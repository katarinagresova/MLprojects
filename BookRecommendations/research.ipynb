{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PR-xIq7_G-HL"
      },
      "source": [
        "## Problem definition\n",
        "\n",
        "Given observations of users past behavior, predict which other things that same user will like. \n",
        "\n",
        "### Input\n",
        "\n",
        "- set of users\n",
        "- set of items\n",
        "- users's ratings for items they interacted with\n",
        "\n",
        "### Outputs\n",
        "\n",
        "- predicted user's ratings for items they haven't interacted with yet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cO_tiZ-2Jk5b"
      },
      "source": [
        "## Common approaches\n",
        "\n",
        "### Collaborative filtering\n",
        "\n",
        "- based on premise that similar similar people like similar things\n",
        "- core data is user-item matrix, also called utility matrix\n",
        "- each number in matrix is representing user's rating of a given item\n",
        "- goal is to fill in the blank spaces in a matrix\n",
        "\n",
        "### Content-based filtering\n",
        "\n",
        "- taking into consideration features of people (age, gender, spoken language, ...) and also features of items"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrYIi0_oILQe"
      },
      "source": [
        "## Recommender pipeline\n",
        "\n",
        "### Pre-processing\n",
        "\n",
        "- transforming user ratings into utility matrix\n",
        "  - rows represents users\n",
        "  - columns represents items\n",
        "  - we can use `scipy.sparse.csr_matrix`\n",
        "- sanity checks\n",
        "  - problem of collaborative filtering - it doesn't perform well on too sparse matrix\n",
        "    - it is important to calculate sparsity of our matrix - total number of ratings that we have divided by number of cells in matrix\n",
        "    - if out data is too sparse (like 0.5%), collaborative filtering might not be the best option\n",
        "  - it is also wort looking at how many new users and new items we have\n",
        "    - collaborative filtering does not work for them\n",
        "- normalization\n",
        "  - we need to normalize ratings be accounting for user and item bias\n",
        "    - optimists: rate everything 4 or 5\n",
        "    - pessimists: rate everything 1 or 2\n",
        "  - mean normalization\n",
        "    - take average rating of an item and subtract that average from every rating of that item\n",
        "    - the same for users - take an average user's rating and subtract it from all ratings that user gave\n",
        "\n",
        "### Model picking\n",
        "\n",
        "- matrix factorization is a commonly used technique in collaborative filtering\n",
        "  - we start with user-item matrix and we factorize it into two latent factor matrices\n",
        "  - we have a user factor matrix and item factor matrix\n",
        "  - we don't actually know what each latent feature represents, but we could imagine that one feature could represent an user that likes indie scary movies from 90s\n",
        "  - once we have these two latent feature matrices, we can reconstruct user-item matrix by taking the inner product of these two latent factor matrices\n",
        "  - this reconstructed matrix will populate empty cells in out original matrix - our predicted ratings\n",
        "- there are many algorithms for matrix factorization\n",
        "  - examples\n",
        "    - Alternating Least Squares (ALS)\n",
        "    - Stochastic Gradient Descent (SGD)\n",
        "    - Singular Value Decomposition (SVD)\n",
        "\n",
        "### Evaluation metric picking\n",
        "\n",
        "- what are we trying to optimize here?\n",
        "- popular metrics for recommenders is `precision@k`\n",
        "  - it looks at top `k` recommendations and calculates what proportion is relevant to an user\n",
        "\n",
        "### Hyperparameter tuning\n",
        "\n",
        "- hyperparameters are external property of a model\n",
        "- we need to try all combinations and find out which gives us the best results\n",
        "- let's say we are using ALS for matrix factorization\n",
        "  - we need to find two hyperparameters\n",
        "    - k (# of factors)\n",
        "    - Î» (regularization parameter)\n",
        "  - goal is to find the hyperparameters that give us the best precision@k (or any other metric that we want to optimize)\n",
        "- there are several ways to do that\n",
        "  - Grid search\n",
        "    - `sklearn.model_selection.GridSearchCV`\n",
        "    - iterates over all combinations of provided values of hyperparameters\n",
        "  - Random search\n",
        "    - `sklearn.model_selection.RandomizedSearchCV`\n",
        "    - evaluates randomly selected combinations of hyperparameters\n",
        "    - this approach is less exhaustive and more effective than grid search\n",
        "  - Sequential Model-based optimization\n",
        "    - it takes into consideration results of previous iterations when picking values of hyperparameters to try in this iteration\n",
        "    - `scikit-optimize`, `hyperopt`, `Metric Optimization Engine`\n",
        "\n",
        "### Model training and Prediction\n",
        "\n",
        "- let's say we tuned our hyperparameters and found ones that give us the best precision\n",
        "- we can train our model with these optimal hyperparameters to get our predicted ratings\n",
        "- we can use these results to generate our recommendations\n",
        "\n",
        "### Post-processing\n",
        "\n",
        "- we have to sort all predicted ratings and get top N\n",
        "- we might wanna filter out items that a user already interacted with\n",
        "- we can also generate item-item recommendations with this infrastructure\n",
        "  - we can apply some similarity metric (e.g. cosine similarity) to get the most similar movies for a given movie\n",
        "  - \"Because you watched movie X, you might also like...\"\n",
        "\n",
        "### Evaluation\n",
        "\n",
        "- the best way to evaluate any recommender is to test it in a wild\n",
        "  - do A/B testing, usability testing, feedback from real users\n",
        "- if that is not possible, we can do off-line evaluation\n",
        "  - in traditional ML, we would split our dataset into two parts - some users would be in training and some in testing\n",
        "  - this is not working for recommender models, because the model will not work if we train it on different user population than the validation set\n",
        "  - for recommenders, we mask random interactions in our matrix \n",
        "  - we pretend that we don't know user's rating of some items so we can compare predicted rating with the actual rating\n",
        "- metrics\n",
        "  - Precision@K\n",
        "    - of the top k recommendations, what proportion is actually relevant?\n",
        "    - trying to minimize number of false positives - irrelevant items, that did get into top k recommendations\n",
        "  - Recall@K\n",
        "    - what proportion of real top k items got into top k recommendations\n",
        "    - trying to minimize number of false negative - relevant items, that didn't get into top k recommendations\n",
        "\n",
        "### Important considerations\n",
        "\n",
        "- interpretability\n",
        "- efficiency and scalability\n",
        "- diversity\n",
        "- serendipity\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPESXGlaz4v5FNgZlcI4EPl",
      "name": "research.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
